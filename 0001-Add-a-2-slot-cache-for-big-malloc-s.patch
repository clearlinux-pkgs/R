From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: Arjan van de Ven <arjan@linux.intel.com>
Date: Sat, 13 Jun 2015 11:27:23 -0400
Subject: [PATCH] Add a 2-slot cache for big malloc()s

R has a pattern of freeing a large vector (usually as part of the GC
step in the vector allocation code) just before .... allocating the same
exact vector again.  (in the higher level this is for temporary values
etc so its sort of logical)

this patch adds a 2-slot cache for such malloc()s, this will avoid a
munmap / tlb fush / mmap / pagefault-for-all-pages cost on the kernel
side.

In addition, there's a thinko in the code around garbage collection. The
code tries to call into the GC when the upcoming allocation doesn't fit
in the memory pools..... even for the large allocations not coming out
of those pools... and those will also by definition not fit.

On the R-benchmark-25.R performance test, this is giving a > 3.2%
improvement.

Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
Signed-off-by: Patrick McCarty <patrick.mccarty@intel.com>
---
 src/main/memory.c | 71 ++++++++++++++++++++++++++++++++++++++++-------
 1 file changed, 61 insertions(+), 10 deletions(-)

diff --git a/src/main/memory.c b/src/main/memory.c
index 2cafc55..f286f5e 100644
--- a/src/main/memory.c
+++ b/src/main/memory.c
@@ -131,6 +131,50 @@ static void gc_error(const char *msg)
 /* These are used in profiling to separate out time in GC */
 int R_gc_running() { return R_in_gc; }
 
+#include <malloc.h>
+/* 2 slot malloc cache + wrappers */
+static void *malloc_cache_entry1;
+static unsigned long malloc_cache_size1;
+static void *malloc_cache_entry2;
+static unsigned long malloc_cache_size2;
+static int malloc_cache_ptr;
+
+static void * malloc_wrapper(size_t size)
+{
+    if (malloc_cache_entry1 && malloc_cache_size1 >= size && malloc_cache_size1 < (size + 8192)) {
+      void *ptr;
+      ptr = malloc_cache_entry1;
+      malloc_cache_entry1 = NULL;
+      malloc_cache_size1 = 0;
+      return ptr;
+    }
+    if (malloc_cache_entry2 && malloc_cache_size2 >= size && malloc_cache_size2 < (size + 8192)) {
+      void *ptr;
+      ptr = malloc_cache_entry2;
+      malloc_cache_entry2 = NULL;
+      malloc_cache_size2 = 0;
+      return ptr;
+    }
+   return malloc(size);
+}
+
+static void free_wrapper(void *ptr)
+{
+  malloc_cache_ptr = (malloc_cache_ptr + 1) & 1;
+
+  if ((malloc_cache_ptr == 0 || malloc_cache_entry1 == NULL) && (malloc_cache_entry2 != NULL)) {
+      free(malloc_cache_entry1);
+      malloc_cache_entry1 = ptr;
+      malloc_cache_size1 = malloc_usable_size(ptr);
+      return;
+  }
+
+  free(malloc_cache_entry2);
+  malloc_cache_entry2 = ptr;
+  malloc_cache_size2 = malloc_usable_size(ptr);
+}
+
+
 #ifdef TESTING_WRITE_BARRIER
 # define PROTECTCHECK
 #endif
@@ -244,7 +288,7 @@ static int gc_pending = 0;
    **** collections it might make sense to also wait before starting
    **** to inhibit releases */
 static int gc_force_wait = 0;
-static int gc_force_gap = 0;
+static int gc_force_gap = 16;
 static Rboolean gc_inhibit_release = FALSE;
 #define FORCE_GC (gc_pending || (gc_force_wait > 0 ? (--gc_force_wait > 0 ? 0 : (gc_force_wait = gc_force_gap, 1)) : 0))
 #else
@@ -1111,7 +1155,7 @@ static void ReleaseLargeFreeVectors()
 		R_GenHeap[node_class].AllocCount--;
 		if (node_class == LARGE_NODE_CLASS) {
 		    R_LargeVallocSize -= size;
-		    free(s);
+		    free_wrapper(s);
 		} else {
 		    custom_node_free(s);
 		}
@@ -2771,16 +2815,16 @@ SEXP allocVector3(SEXPTYPE type, R_xlen_t length, R_allocator_t *allocator)
     old_R_VSize = R_VSize;
 
     /* we need to do the gc here so allocSExp doesn't! */
-    if (FORCE_GC || NO_FREE_NODES() || VHEAP_FREE() < alloc_size) {
-	R_gc_internal(alloc_size);
-	if (NO_FREE_NODES())
-	    mem_err_cons();
-	if (VHEAP_FREE() < alloc_size)
-	    mem_err_heap(size);
-    }
 
     if (size > 0) {
 	if (node_class < NUM_SMALL_NODE_CLASSES) {
+            if (FORCE_GC || NO_FREE_NODES() || VHEAP_FREE() < alloc_size) {
+        	R_gc_internal(alloc_size);
+        	if (NO_FREE_NODES())
+          	    mem_err_cons();
+        	if (VHEAP_FREE() < alloc_size)
+          	    mem_err_heap(size);
+            }
 	    CLASS_GET_FREE_NODE(node_class, s);
 #if VALGRIND_LEVEL > 1
 	    VALGRIND_MAKE_MEM_UNDEFINED(STDVEC_DATAPTR(s), actual_size);
@@ -2803,7 +2847,7 @@ SEXP allocVector3(SEXPTYPE type, R_xlen_t length, R_allocator_t *allocator)
 		   indexable by size_t. - TK */
 		mem = allocator ?
 		    custom_node_alloc(allocator, hdrsize + size * sizeof(VECREC)) :
-		    malloc(hdrsize + size * sizeof(VECREC));
+		    malloc_wrapper(hdrsize + size * sizeof(VECREC));
 		if (mem == NULL) {
 		    /* If we are near the address space limit, we
 		       might be short of address space.  So return
@@ -2852,6 +2896,13 @@ SEXP allocVector3(SEXPTYPE type, R_xlen_t length, R_allocator_t *allocator)
 	SET_TYPEOF(s, type);
     }
     else {
+        if (FORCE_GC || NO_FREE_NODES() || VHEAP_FREE() < alloc_size) {
+        	R_gc_internal(alloc_size);
+        	if (NO_FREE_NODES())
+          	    mem_err_cons();
+        	if (VHEAP_FREE() < alloc_size)
+          	    mem_err_heap(size);
+        }
 	GC_PROT(s = allocSExpNonCons(type));
 	SET_STDVEC_LENGTH(s, (R_len_t) length);
     }

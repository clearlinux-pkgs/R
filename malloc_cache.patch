Add a 2-slot cache for big malloc()s

R has a pattern of freeing a large vector (usually as part of the GC step in the vector allocation code)
just before .... allocating the same exact vector again.
(in the higher level this is for temporary values etc so its sort of logical)

this patch adds a 2-slot cache for such malloc()s, this will avoid a munmap / tlb fush / mmap / pagefault-for-all-pages
cost on the kernel side.

In addition, there's a thinko in the code around garbage collection. The code tries to call into the GC when
the upcoming allocation doesn't fit in the memory pools..... even for the large allocations not coming out
of those pools... and those will also by definition not fit.


On the R-benchmark-25.R performance test, this is giving a > 3.2% improvement.


Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>

--- R-3.2.0/src/main/memory.c.org	2015-06-13 11:02:53.455873966 -0400
+++ R-3.2.0/src/main/memory.c	2015-06-13 20:34:28.196130713 -0400
@@ -119,6 +119,50 @@
 static Rboolean R_in_gc = FALSE;
 int R_gc_running() { return R_in_gc; }
 
+#include <malloc.h>
+/* 2 slot malloc cache + wrappers */
+static void *malloc_cache_entry1;
+static unsigned long malloc_cache_size1;
+static void *malloc_cache_entry2;
+static unsigned long malloc_cache_size2;
+static int malloc_cache_ptr;
+
+static void * malloc_wrapper(size_t size)
+{
+    if (malloc_cache_entry1 && malloc_cache_size1 >= size && malloc_cache_size1 < (size + 8192)) {
+      void *ptr;
+      ptr = malloc_cache_entry1;
+      malloc_cache_entry1 = NULL;
+      malloc_cache_size1 = 0;
+      return ptr;
+    }
+    if (malloc_cache_entry2 && malloc_cache_size2 >= size && malloc_cache_size2 < (size + 8192)) {
+      void *ptr;
+      ptr = malloc_cache_entry2;
+      malloc_cache_entry2 = NULL;
+      malloc_cache_size2 = 0;
+      return ptr;
+    }
+   return malloc(size);
+}
+
+static void free_wrapper(void *ptr)
+{
+  malloc_cache_ptr = (malloc_cache_ptr + 1) & 1;
+
+  if ((malloc_cache_ptr == 0 || malloc_cache_entry1 == NULL) && (malloc_cache_entry2 != NULL)) {
+      free(malloc_cache_entry1);
+      malloc_cache_entry1 = ptr;
+      malloc_cache_size1 = malloc_usable_size(ptr);
+      return;
+  }
+
+  free(malloc_cache_entry2);
+  malloc_cache_entry2 = ptr;
+  malloc_cache_size2 = malloc_usable_size(ptr);
+}
+
+
 #ifdef TESTING_WRITE_BARRIER
 # define PROTECTCHECK
 #endif
@@ -1052,7 +1052,7 @@
 		R_GenHeap[node_class].AllocCount--;
 		if (node_class == LARGE_NODE_CLASS) {
 		    R_LargeVallocSize -= size;
-		    free(s);
+		    free_wrapper(s);
 		} else {
 		    custom_node_free(s);
 		}
@@ -2571,16 +2615,16 @@
     old_R_VSize = R_VSize;
 
     /* we need to do the gc here so allocSExp doesn't! */
-    if (FORCE_GC || NO_FREE_NODES() || VHEAP_FREE() < alloc_size) {
-	R_gc_internal(alloc_size);
-	if (NO_FREE_NODES())
-	    mem_err_cons();
-	if (VHEAP_FREE() < alloc_size)
-	    mem_err_heap(size);
-    }
 
     if (size > 0) {
 	if (node_class < NUM_SMALL_NODE_CLASSES) {
+            if (FORCE_GC || NO_FREE_NODES() || VHEAP_FREE() < alloc_size) {
+        	R_gc_internal(alloc_size);
+        	if (NO_FREE_NODES())
+          	    mem_err_cons();
+        	if (VHEAP_FREE() < alloc_size)
+          	    mem_err_heap(size);
+            }
 	    CLASS_GET_FREE_NODE(node_class, s);
 #if VALGRIND_LEVEL > 1
 	    VALGRIND_MAKE_MEM_UNDEFINED(DATAPTR(s), actual_size);
@@ -2602,7 +2646,7 @@
 	    if (size < (R_SIZE_T_MAX / sizeof(VECREC)) - hdrsize) { /*** not sure this test is quite right -- why subtract the header? LT */
 		mem = allocator ?
 		    custom_node_alloc(allocator, hdrsize + size * sizeof(VECREC)) :
-		    malloc(hdrsize + size * sizeof(VECREC));
+		    malloc_wrapper(hdrsize + size * sizeof(VECREC));
 		if (mem == NULL) {
 		    /* If we are near the address space limit, we
 		       might be short of address space.  So return
@@ -2664,6 +2708,13 @@
 	TYPEOF(s) = type;
     }
     else {
+        if (FORCE_GC || NO_FREE_NODES() || VHEAP_FREE() < alloc_size) {
+        	R_gc_internal(alloc_size);
+        	if (NO_FREE_NODES())
+          	    mem_err_cons();
+        	if (VHEAP_FREE() < alloc_size)
+          	    mem_err_heap(size);
+        }
 	GC_PROT(s = allocSExpNonCons(type));
 	SET_SHORT_VEC_LENGTH(s, (R_len_t) length);
     }
--- R-3.2.0/src/main/memory.c~	2015-06-13 20:34:28.000000000 -0400
+++ R-3.2.0/src/main/memory.c	2015-06-13 22:46:32.542062756 -0400
@@ -275,7 +275,7 @@
    **** collections it might make sense to also wait before starting
    **** to inhibit releases */
 static int gc_force_wait = 0;
-static int gc_force_gap = 0;
+static int gc_force_gap = 16;
 static Rboolean gc_inhibit_release = FALSE;
 #define FORCE_GC (gc_force_wait > 0 ? (--gc_force_wait > 0 ? 0 : (gc_force_wait = gc_force_gap, 1)) : 0)
 #else
